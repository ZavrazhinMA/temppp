{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ee8ec3-795d-4268-a451-95ad9a8c55e4",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f9c5a88-d7e7-44fd-a41e-7caf568d3d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets scikit-learn pandas\n",
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1a4c10-312f-4218-b5a7-23ecf2862534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c374e-69a9-476c-98de-3902e57323ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f0e749-8f9b-40e3-b960-794c8074ff17",
   "metadata": {},
   "source": [
    "======================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0a1d42-5170-4efe-ac31-98ba46b8ac87",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f49f59f-9a57-4078-928b-516ae1d5fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# для проверки пайплайна\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_df = pd.DataFrame(dataset['train']).sample(10000, random_state=42).reset_index(drop=True)\n",
    "test_df = pd.DataFrame(dataset['test']).sample(2000, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0eb31bd-e977-42b0-8a4a-695b569aa805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e75fbe3f-c20c-467b-80b1-e0dcf1482583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dumb is as dumb does, in this thoroughly unint...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I dug out from my garage some old musicals and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After watching this movie I was honestly disap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This movie was nominated for best picture but ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just like Al Gore shook us up with his painful...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>OK, here is my personal list of top Nicktoons ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>The surprise nominee of this year's Best Anima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>As others have said, \"No, Luciano\" is a more a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Tierney's an authentic tough guy, but this mov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>I first heard about this film about 20 years a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     Dumb is as dumb does, in this thoroughly unint...      0\n",
       "1     I dug out from my garage some old musicals and...      1\n",
       "2     After watching this movie I was honestly disap...      0\n",
       "3     This movie was nominated for best picture but ...      1\n",
       "4     Just like Al Gore shook us up with his painful...      1\n",
       "...                                                 ...    ...\n",
       "9995  OK, here is my personal list of top Nicktoons ...      1\n",
       "9996  The surprise nominee of this year's Best Anima...      1\n",
       "9997  As others have said, \"No, Luciano\" is a more a...      0\n",
       "9998  Tierney's an authentic tough guy, but this mov...      0\n",
       "9999  I first heard about this film about 20 years a...      1\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b6f800-e866-45da-9a4a-0c2718718346",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=512, text_col=\"text\", label_col=\"label\"):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.text_col = text_col\n",
    "        self.label_col = label_col\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, self.text_col]\n",
    "        label = self.df.loc[idx, self.label_col]\n",
    "\n",
    "        tokens = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "  \n",
    "        item = {key: val.squeeze() for key, val in tokens.items()}\n",
    "        item[self.label_col] = torch.tensor(label, dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b370954-8596-4ad5-9f13-be2dca68cba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, tokenizer, text_column=\"text\", label_column=\"label\", max_length=512):\n",
    "    \n",
    "    dataset = Dataset.from_pandas(df[[text_column, label_column]])\n",
    "    \n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(\n",
    "            example[text_column], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=max_length\n",
    "        )\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns([text_column])\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e089b530-0573-4875-ad78-2882652fb826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_report(model, tokenizer, df, text_column=\"text\", label_column=\"label\", max_length=512, batch_size=32):\n",
    "    predictions = []\n",
    "    labels = df[label_column].tolist()\n",
    "    \n",
    "    for i in tqdm(range(0, len(df), batch_size)):\n",
    "        batch_texts = df[text_column].iloc[i:i + batch_size].tolist()\n",
    "        \n",
    "        tokens = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**tokens)\n",
    "            batch_predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            predictions.extend(batch_predictions)\n",
    "        del tokens\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    report = classification_report(labels, predictions, target_names=[\"Class 0\", \"Class 1\"])\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c5738-7461-4653-a081-7d18c3b17f9c",
   "metadata": {},
   "source": [
    "==============================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab2f675a-c0b7-4b6c-941e-26169b8feaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    # accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average=\"weighted\")\n",
    "    recall = recall_score(labels, predictions, average=\"weighted\")\n",
    "    \n",
    "    return {\"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db5cd50c-68be-4f0b-b0af-c4a91cd3b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используемое устройство: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40206f40-9770-47ba-91bd-1c2a2ab15c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/LaBSE-en-ru and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"cointegrated/LaBSE-en-ru\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78a08771-51d0-4448-96dc-c4204ef920bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc76d69b5e9f449ba3ca228f9fa65cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.54      0.65      0.59      1040\n",
      "     Class 1       0.51      0.39      0.44       960\n",
      "\n",
      "    accuracy                           0.53      2000\n",
      "   macro avg       0.52      0.52      0.52      2000\n",
      "weighted avg       0.52      0.53      0.52      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_classification_report(model, tokenizer, test_df, text_column=\"text\", label_column=\"label\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd81c99-9953-421d-b313-b09e2e7dcfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df, tokenizer, max_length=512)\n",
    "test_dataset = TextDataset(test_df, tokenizer, max_length=512)\n",
    "# train_dataset = prepare_dataset(train_df, tokenizer, text_column=\"text\", label_column=\"label\", max_length=512)\n",
    "# test_dataset = prepare_dataset(test_df, tokenizer, text_column=\"text\", label_column=\"label\", max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34eb3afd-d986-4f54-8377-88f090071442",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                      # Директория для сохранения результатов\n",
    "    eval_strategy=\"epoch\",                 # Оценка модели на каждой эпохе\n",
    "    learning_rate=2e-5,                          # Скорость обучения\n",
    "    per_device_train_batch_size=32,              # Размер батча для обучения\n",
    "    per_device_eval_batch_size=32,               # Размер батча для валидации\n",
    "    num_train_epochs=10,                         # Количество эпох\n",
    "    weight_decay=0.00001,                           # Коэффициент регуляризации\n",
    "    logging_dir='./logs',                        # Директория для логов (TensorBoard)\n",
    "    logging_steps=50,                           # Логирование каждые 500 шагов\n",
    "    load_best_model_at_end=True,                 # Загрузка лучшей модели по окончанию обучения\n",
    "    save_total_limit=2,                          # Сохраняем только 2 лучшие модели\n",
    "    metric_for_best_model=\"f1\",                  # Ключевая метрика для выбора лучшей модели\n",
    "    greater_is_better=True,                      # Лучшая модель — та, где метрика больше\n",
    "    save_strategy=\"epoch\",                       # Сохраняем модель на каждой эпохе\n",
    "    report_to=\"tensorboard\",                     # Используем TensorBoard для логирования\n",
    "    optim=\"adamw_torch\",                         # Явно указываем AdamW как оптимизатор\n",
    "    warmup_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40151f90-1d13-410f-867d-64e3cccae051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_9156\\100174452.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='3130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   9/3130 01:24 < 10:31:02, 0.08 it/s, Epoch 0.03/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# data_collator=data_collator\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Запуск тренировки\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2123\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2124\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2125\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2126\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2127\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2479\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2473\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2474\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2477\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2479\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2480\u001b[0m ):\n\u001b[0;32m   2481\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2482\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,                 # Тренировочный датасет\n",
    "    eval_dataset=test_dataset,                    # Валидационный датасет\n",
    "    tokenizer=tokenizer,                         # Токенизатор\n",
    "    compute_metrics=compute_metrics,             # Функция вычисления метрик\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # Ранняя остановка (2 эпохи)\n",
    "    # data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Запуск тренировки\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78d10d-6861-44a0-890d-050e76cec19a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
